#!/usr/bin/env python3
# -*- coding: utf-8 -*-
'''
Scan module
'''

import os
import configparser
import sys
import json
import time
import pdfkit
import requests

from core.config import cfg

CFG = configparser.ConfigParser()
CFG.read('config.ini')

class ScanModule:
    '''Class ScanModule'''
    @staticmethod
    def output_array_all_formats(array, file):
        '''Function to output '''
        output = open('%s.json' % file, 'w+')
        output.write(str(json.dumps(array, indent=4, sort_keys=True)) + '\r\n')
        output.close()

        output = open('%s.txt' % file, 'w+')
        for item in array:
            output.write(item+'\r\n')
        output.close()

    @staticmethod
    def get_ssl_scan():
        '''Function to get the pdf file of a SSL Labs scan'''
        output_dir = '{}/{}'.format(cfg.get().output_dir, 'SSLSCAN')
        author = CFG['DEFAULT']['AUTHOR']

        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        res = requests.Response()
        res_api = requests.Response()
        res_api_json = None
        while res.content is None or 'Please wait...' in str(res.content):
            print('Please wait, test in progress on SSL Labs...')
            res_api = requests.get('https://api.dev.ssllabs.com/api/v3/analyze?host={}'
                                   .format(cfg.get().host))
            res_api_json = json.loads(res_api.content.decode("utf-8"))

            res = requests.get('https://www.ssllabs.com/ssltest/analyze.html?d={}\
                        &latest&ignoreMismatch=on&hideResults=on'.format(cfg.get().host))

            if res.status_code != 200 or res_api.status_code != 200:
                raise Exception('SSL Labs is down')
            time.sleep(15)

        file = open('{}/SSL-Labs-{}.json'.format(output_dir, cfg.get().host), 'w+')
        file.write(str(json.dumps(res_api_json, indent=4, sort_keys=True)))
        file.close()

        pdfkit.from_url('https://www.ssllabs.com/ssltest/analyze.html?d={}\
                        &latest&ignoreMismatch=on&hideResults=on'
                        .format(cfg.get().host), '{}/{}-{}.pdf'
                        .format(output_dir, cfg.get().host, author))

    @staticmethod
    def get_internet_archives(recursively=False):
        '''Function to get internet archives from the wayback machine'''
        output_dir = '{}/{}'.format(cfg.get().output_dir, 'WORK')
        author = CFG['DEFAULT']['AUTHOR']

        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        print('Starting an internet archives scan on {}...'.format(cfg.get().host_without_www))
        req = requests.get('http://web.archive.org/cdx/search/cdx?url={}*&output=json'
                           .format(cfg.get().host_without_www))

        if req.status_code != 200:
            print('error: Internet Archives API is offline')
            sys.exit(0)

        print('Internet Archives API is online')
        req = json.loads(req.content)

        print('=== Internet Archives results ===')
        links = []

        if req is None:
            print('No informations available on the internet archives for: {}'
                  .format(cfg.get().host_without_www))
            sys.exit(0)

        for jobject in req:
            if jobject[2] not in links and jobject[2] != "original":
                if str(cfg.get().host_without_www) in jobject[2]:
                    links.append(jobject[2])
                    print(jobject[2])
        if recursively:
            print('all available links dumped, checking the dumped links...')
            for node in links:
                # Pass the incorrect links.
                try:
                    print('Checking {} ...'.format(node))
                    req = requests.get(node)
                    soup = BeautifulSoup(req.content, 'html.parser', from_encoding="iso-8859-1")
                    for link in soup.findAll('a', attrs={'href': re.compile("^https?://")}):
                        if data['host_without_www'] in link and link is not links:
                            print(link.get('href'))
                            links.append(link.get('href'))
                except:
                    pass
        print('saving the results...')

        ScanModule.output_array_all_formats(links, '{}/internet-archives_{}.{}'
                                            .format(output_dir, cfg.get().host_without_www, author))
        print('=================================')
